{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preparation\n",
    "\n",
    ":::{admonition} Course Website \n",
    "http://54.243.252.9/ce-5319-webroot/ \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readings\n",
    "\n",
    "1. [Burkov, A. (2019) The One Hundred Page Machine Learning Book](http://ema.cri-info.cm/wp-content/uploads/2019/07/2019BurkovTheHundred-pageMachineLearning.pdf) Required Textbook\n",
    "\n",
    "2. [Rashid, Tariq. (2016) Make Your Own Neural Network. Kindle Edition. ](https://www.goodreads.com/en/book/show/29746976-make-your-own-neural-network) Required Textbook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson: Data Preparation\n",
    "Machine Learning by Example for Civil Engineers\n",
    "1. Introduction to Data Preparation\n",
    "Importance of data preprocessing in machine learning\n",
    "Common challenges in raw datasets (e.g., missing values, inconsistencies, noise)\n",
    "Overview of data pipeline stages: Acquisition → Cleaning → Transformation → Storage\n",
    "2. Accessing and Retrieving Data\n",
    "Remote Data Access\n",
    "Using requests to retrieve data from remote servers (REST APIs, static files)\n",
    "Handling authentication & API keys (e.g., using requests with headers)\n",
    "Working with different data formats: JSON, XML, CSV\n",
    "Database Connections\n",
    "Introduction to SQL databases (sqlite3, SQLAlchemy)\n",
    "Accessing cloud-based databases (PostgreSQL, MySQL)\n",
    "Querying structured data with SQL\n",
    "3. Data Cleaning and Pruning\n",
    "Identifying and Handling Missing Data\n",
    "Detecting missing values (pandas.isnull(), df.info())\n",
    "Strategies: Dropping vs. Imputation (mean, median, mode)\n",
    "Handling Duplicate Records\n",
    "Identifying and removing duplicates (df.drop_duplicates())\n",
    "Dealing with Outliers\n",
    "Statistical methods: Z-score, IQR (Interquartile Range)\n",
    "Visual detection with boxplots\n",
    "Filtering and Selecting Relevant Data\n",
    "Criteria-based pruning (e.g., removing negative values, extreme values)\n",
    "Dropping irrelevant columns/features\n",
    "4. Data Encoding and Decoding\n",
    "Handling Categorical Data\n",
    "One-hot encoding (pandas.get_dummies())\n",
    "Label encoding vs. Ordinal encoding (sklearn.preprocessing.LabelEncoder)\n",
    "Working with Text Data\n",
    "Converting text to numerical representations (TF-IDF, word embeddings)\n",
    "Tokenization and stemming\n",
    "Date and Time Processing\n",
    "Converting timestamps to usable features (datetime module)\n",
    "Extracting components (day, month, seasonality effects)\n",
    "5. Feature Scaling and Normalization\n",
    "Why Scaling is Important?\n",
    "Impact on machine learning models (e.g., gradient descent performance)\n",
    "Normalization vs. Standardization\n",
    "Min-Max scaling (sklearn.preprocessing.MinMaxScaler)\n",
    "Standardization (StandardScaler)\n",
    "Handling Skewed Data\n",
    "Log transformation, power transformation\n",
    "6. Data Transformation and Augmentation\n",
    "Feature Engineering for Civil Engineering Data\n",
    "Creating new features (e.g., water flow rates from raw sensor readings)\n",
    "Combining multiple features (e.g., aggregating rainfall over time)\n",
    "Data Augmentation for Small Datasets\n",
    "Synthetic data generation techniques\n",
    "Augmentation in image-based models (e.g., rotating, flipping images for CNNs)\n",
    "7. Exporting and Storing Processed Data\n",
    "Saving Cleaned Data\n",
    "Writing to CSV, Excel (df.to_csv(), df.to_excel())\n",
    "Saving as SQL tables (df.to_sql())\n",
    "Data Serialization\n",
    "JSON and Pickle for storing processed datasets\n",
    "Best Practices for Reproducibility\n",
    "Versioning data with metadata logs\n",
    "Automating data preparation with pipelines (scikit-learn Pipelines)\n",
    "8. Case Study: Preparing Civil Engineering Data for ML\n",
    "Example: Processing rainfall-runoff time series data\n",
    "Example: Cleaning and encoding geospatial data for infrastructure planning\n",
    "Example: Normalizing soil property data for ML-based classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Accessing and Retrieving Data\n",
    "Example 1: Fetching Data from a Remote API (Using requests)\n",
    "This example demonstrates how to retrieve rainfall data from a hypothetical REST API."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the remote data source (example API)\n",
    "url = \"https://example.com/api/rainfall\"\n",
    "\n",
    "# Fetch data using requests\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()  # Assuming JSON format\n",
    "    df = pd.DataFrame(data)\n",
    "    print(df.head())  # Display first few rows\n",
    "else:\n",
    "    print(\"Failed to fetch data:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2: Connecting to a SQL Database (sqlite3)\n",
    "Students will learn how to query structured datasets stored in SQLite."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to database (or create one)\n",
    "conn = sqlite3.connect(\"civil_engineering_data.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Query rainfall data\n",
    "query = \"SELECT * FROM rainfall_data WHERE year >= 2000\"\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Display sample data\n",
    "df.head()\n",
    "\n",
    "# Close connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Data Cleaning and Pruning\n",
    "Example 3: Handling Missing Data (pandas)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a sample dataset\n",
    "data = {\n",
    "    \"Station\": [\"A\", \"B\", \"C\", \"D\"],\n",
    "    \"Rainfall (mm)\": [10.2, np.nan, 35.4, 20.1],  # Missing value at index 1\n",
    "    \"Temperature (°C)\": [22.5, 23.1, np.nan, 21.8]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Fill missing values with mean\n",
    "df_filled = df.fillna(df.mean())\n",
    "\n",
    "print(df_filled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 4: Removing Outliers Using Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Generate sample data\n",
    "df = pd.DataFrame({\"Flow Rate (L/s)\": [1.2, 1.3, 1.1, 50.0, 1.4, 1.2]})\n",
    "\n",
    "# Compute Z-scores\n",
    "df[\"Z-score\"] = stats.zscore(df[\"Flow Rate (L/s)\"])\n",
    "\n",
    "# Filter out extreme outliers\n",
    "df_cleaned = df[df[\"Z-score\"].abs() < 3]\n",
    "\n",
    "print(df_cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Data Encoding and Transformation\n",
    "Example 5: One-Hot Encoding of Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Sample categorical data\n",
    "df = pd.DataFrame({\"Material\": [\"Concrete\", \"Steel\", \"Wood\", \"Concrete\"]})\n",
    "\n",
    "# One-hot encode material type\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoded = encoder.fit_transform(df[[\"Material\"]])\n",
    "\n",
    "# Convert back to DataFrame\n",
    "df_encoded = pd.DataFrame(encoded, columns=encoder.get_feature_names_out())\n",
    "\n",
    "print(df_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Example 6: Scaling Features with MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample data\n",
    "df = pd.DataFrame({\"Moisture (%)\": [20, 35, 50, 65, 80]})\n",
    "\n",
    "# Normalize between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print(df_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Exporting and Storing Processed Data\n",
    "Example 7: Saving Cleaned Data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.to_csv(\"cleaned_data.csv\", index=False)\n",
    "print(\"Data saved to cleaned_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Case Study: Preparing Hydrological Data\n",
    "Example 8: Full Data Preparation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load data\n",
    "df = pd.read_csv(\"rainfall_data.csv\")\n",
    "\n",
    "# Step 2: Remove missing values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Step 3: Convert categorical features (station type)\n",
    "df_encoded = pd.get_dummies(df_cleaned, columns=[\"Station Type\"])\n",
    "\n",
    "# Step 4: Normalize flow rates\n",
    "scaler = MinMaxScaler()\n",
    "df_encoded[\"Flow Rate (scaled)\"] = scaler.fit_transform(df_encoded[[\"Flow Rate (L/s)\"]])\n",
    "\n",
    "# Step 5: Save processed data\n",
    "df_encoded.to_csv(\"processed_rainfall_data.csv\", index=False)\n",
    "\n",
    "print(\"Processed dataset ready!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
