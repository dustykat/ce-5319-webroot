{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53539b67-dcc8-4ebc-90cc-ad2699816b00",
   "metadata": {},
   "source": [
    "# 13. Feature Engineering\n",
    "\n",
    ":::{admonition} Course Website \n",
    "http://54.243.252.9/ce-5319-webroot/ \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e657b097-aa2a-4dba-bd5e-eefea37e0dff",
   "metadata": {},
   "source": [
    "## References\n",
    "1. [Burkov, A. (2019) The One Hundred Page Machine Learning Book](http://ema.cri-info.cm/wp-content/uploads/2019/07/2019BurkovTheHundred-pageMachineLearning.pdf) - Required Textbook.\n",
    "2. [Molnar, C. (2022). Interpretable Machine Learning: A Guide for Making Black Box Models Explainable.](https://christophm.github.io/interpretable-ml-book/) This book provides insights into understanding AI-generated models and results, reinforcing the importance of interpretability in human-AI collaboration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca3594f-14cc-4507-9927-39102fee69cc",
   "metadata": {},
   "source": [
    "## Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04252487-04c1-4544-808b-17c09320661f",
   "metadata": {},
   "source": [
    "## Feature Engineering in Machine Learning\n",
    "In traditional statistical modeling, particularly in regression analysis, the process of selecting explanatory variables was a crucial step. Analysts would identify relevant independent variables that best explained variability in the dependent variable, ensuring they met assumptions like linearity, independence, and minimal multicollinearity. This process—variable selection—focused on choosing and transforming numerical features to maximize predictive accuracy while maintaining interpretability.\n",
    "\n",
    "In Machine Learning (ML), the concept of feature engineering extends beyond traditional variable selection. ML models can handle diverse feature types, including:\n",
    "\n",
    "- Real-valued variables (e.g., temperature, stock prices, rainfall)\n",
    "- Categorical variables (e.g., country, product category, job title)\n",
    "- Text features (e.g., words, phrases, document embeddings)\n",
    "- Image features (e.g., pixel values, edge maps, deep learning embeddings)\n",
    "- Time series data (e.g., sequences, lagged values)\n",
    "- Graph-based features (e.g., social network connections, road network distances)\n",
    "\n",
    "In modern ML applications, features are not necessarily pre-defined by human experts; they can be automatically extracted, transformed, and optimized through computational techniques. This is particularly true for deep learning, where models learn hierarchical representations of raw data (e.g., convolutional layers for images or word embeddings for text).\n",
    "\n",
    "Regardless of the model type, the core goal of feature engineering remains the same: to create representations of input data that improve predictive performance. This involves:\n",
    "\n",
    "- Feature Selection – Identifying the most relevant features.\n",
    "- Feature Transformation – Encoding categorical variables, normalizing numerical variables, or applying polynomial expansions.\n",
    "- Feature Extraction – Automatically learning new features, such as word embeddings for text or principal components for dimensionality reduction.\n",
    "- Feature Construction – Combining raw variables into more meaningful representations, such as aggregating time-based trends or deriving interaction terms.\n",
    "\n",
    "The ability of ML models to process and integrate mixed data types (numbers, text, images, etc.) sets them apart from traditional statistical models. Ultimately, effective feature engineering can dramatically enhance model accuracy and efficiency, often proving to be more important than the choice of algorithm itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ec120d-7b49-474f-9450-6714218a7255",
   "metadata": {},
   "source": [
    "## Feature Engineering Methods by Data Type\n",
    "\n",
    "### Numeric Data (Continuous or Discrete)\n",
    "These methods are typically used for structured numerical data, reducing dimensionality, extracting patterns, or transforming raw features.\n",
    "\n",
    "#### Dimensionality Reduction & Projection Methods\n",
    "- Principal Component Analysis (PCA) – Projects data into uncorrelated principal components.\n",
    "- Empirical Orthogonal Functions (EOF) – Similar to PCA, commonly used in climate and geospatial analysis.\n",
    "- Independent Component Analysis (ICA) – Separates independent signals from mixed signals.\n",
    "- Factor Analysis – Extracts latent variables that explain observed variance.\n",
    "- t-SNE (t-Distributed Stochastic Neighbor Embedding) – Nonlinear dimensionality reduction for visualization.\n",
    "- UMAP (Uniform Manifold Approximation and Projection) – Similar to t-SNE but preserves more global structure.\n",
    "#### Transformation & Scaling\n",
    "- Log Transform – Helps stabilize variance and handle skewed distributions.\n",
    "- Box-Cox Transform – Used for normalizing skewed data.\n",
    "- Min-Max Scaling – Scales features to a specific range (e.g., [0,1]).\n",
    "- Z-Score Normalization – Standardizes features to have mean 0 and standard deviation 1.\n",
    "- Binning (Discretization) – Converts continuous variables into categorical bins (e.g., quantile bins).\n",
    "#### Feature Extraction & Construction\n",
    "- Fourier Transform / Wavelet Transform – Used in signal processing to extract frequency components.\n",
    "- Autoregressive Features (AR, MA, ARMA, ARIMA) – Common in time series modeling.\n",
    "- Polynomial Features – Generates polynomial terms for regression models.\n",
    "- Interaction Features – Combines two or more features multiplicatively or additively.\n",
    "\n",
    "### Categorical Data (Nominal or Ordinal)\n",
    "Methods for converting categorical variables into numerical representations.\n",
    "\n",
    "#### Encoding Methods\n",
    "- One-Hot Encoding – Converts categorical variables into binary vectors.\n",
    "- Ordinal Encoding – Assigns integer values based on category order.\n",
    "- Target Encoding (Mean Encoding) – Replaces categories with their mean response variable value.\n",
    "- Frequency Encoding – Replaces categories with their occurrence frequency.\n",
    "- Embedding Representation – Uses dense vector representations (learned in deep learning models).\n",
    "#### Grouping & Aggregation\n",
    "- Rare Category Grouping – Groups infrequent categories into an “Other” category.\n",
    "- Domain-Specific Binning – Groups categorical variables based on meaningful criteria (e.g., age groups).\n",
    "\n",
    "### Text Data\n",
    "Methods for extracting numerical features from text.\n",
    "\n",
    "#### Traditional Text Feature Extraction\n",
    "- Bag of Words (BoW) – Represents text as a sparse count matrix.\n",
    "- TF-IDF (Term Frequency-Inverse Document Frequency) – Weighs words by importance in a document corpus.\n",
    "- N-grams – Extracts word sequences (bigrams, trigrams, etc.).\n",
    "- Embedding & Deep Learning-Based Methods\n",
    "- Word2Vec (CBOW, Skip-Gram) – Learns word embeddings from text.\n",
    "- GloVe (Global Vectors for Word Representation) – Generates word embeddings using co-occurrence statistics.\n",
    "- FastText – A variation of Word2Vec that includes subword information.\n",
    "- BERT (Bidirectional Encoder Representations from Transformers) – Context-aware deep learning embeddings.\n",
    "- Sentence Transformers (SBERT, T5, etc.) – Generates sentence-level embeddings.\n",
    "#### Advanced NLP Feature Engineering\n",
    "- Topic Modeling (LDA, Latent Semantic Analysis) – Extracts underlying topics from a corpus.\n",
    "- Sentiment Analysis Scores – Maps text to a sentiment scale.\n",
    "- Named Entity Recognition (NER) – Identifies entities like names, locations, and organizations.\n",
    "\n",
    "### Image Data\n",
    "Methods for feature extraction from image-based data.\n",
    "\n",
    "#### Traditional Feature Extraction\n",
    "- Histogram of Oriented Gradients (HOG) – Captures edge directions and gradients.\n",
    "- Scale-Invariant Feature Transform (SIFT) – Extracts keypoints for object recognition.\n",
    "- Local Binary Patterns (LBP) – Captures texture patterns.\n",
    "#### Deep Learning-Based Feature Extraction\n",
    "- Convolutional Neural Networks (CNNs) – Extract hierarchical image features.\n",
    "- Pretrained CNN Embeddings (ResNet, VGG, Inception, EfficientNet) – Uses deep models trained on large datasets to extract features.\n",
    "- Autoencoders – Learns compressed latent representations of images.\n",
    "- GAN-Based Feature Learning (StyleGAN, BigGAN) – Generates and modifies image features.\n",
    "\n",
    "### Graph-Based Data (Networks, Relationships)\n",
    "Methods for encoding structured graph relationships.\n",
    "\n",
    "#### Graph Feature Extraction\n",
    "- Node Degree, Betweenness Centrality, Closeness Centrality – Captures node importance in a graph.\n",
    "- Eigenvector Centrality (PageRank) – Measures influence of a node in a network.\n",
    "- Graph Kernels (Weisfeiler-Lehman, Shortest Path Kernel) – Measures graph similarity.\n",
    "#### Graph Embedding Methods\n",
    "- Node2Vec – Learns node representations via random walks.\n",
    "- DeepWalk – Similar to Word2Vec but for graphs.\n",
    "- Graph Convolutional Networks (GCNs) – Extends deep learning to graphs.\n",
    "- Graph Attention Networks (GATs) – Uses attention mechanisms for node embeddings.\n",
    "\n",
    "### Mixed Data Types (Multimodal Learning)\n",
    "Methods for combining heterogeneous data sources (text, images, structured data, etc.).\n",
    "\n",
    "#### Feature Fusion Approaches\n",
    "- Concatenation-Based Fusion – Directly concatenates numerical, categorical, text, and image embeddings.\n",
    "- Autoencoder-Based Fusion – Learns a unified representation from multiple modalities.\n",
    "- Transformer-Based Multimodal Learning – Models like CLIP (Contrastive Language-Image Pretraining) combine vision and text.\n",
    "- Graph Neural Networks (GNNs) for Multimodal Data – Represents heterogeneous data in a structured way.\n",
    "\n",
    ":::{admonition} Importance of Feature Engineering\n",
    "Feature engineering a critical step in Machine Learning. Whether working with structured numerical data, categorical data, or unstructured data (text, images, graphs), choosing the right transformation, encoding, or embedding method can significantly improve predictive performance.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5810c6a-4a6a-49d2-a4fc-a2e9ebd7176b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Common Environment",
   "language": "python",
   "name": "python-my-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
