{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Validation\n",
    "\n",
    ":::{admonition} Course Website \n",
    "http://54.243.252.9/ce-5319-webroot/ \n",
    ":::\n",
    "\n",
    "Once a model is built and trained, validation is the next phase (we called it testing in the examples).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation\n",
    "\n",
    ">Cross-validation is a statistical method used to estimate the skill of machine learning models. It is commonly used in applied machine learning to compare and select a model for a given predictive modeling problem because it is easy to understand, easy to implement, and results in skill estimates that generally have a lower bias than other methods.\n",
    "\n",
    "\n",
    "Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample.\n",
    "\n",
    "The procedure has a single parameter called **k** that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called **k-fold cross-validation**. When a specific value for **k** is chosen, it may be used in place of **k** in the reference to the model, such as **k=10** becoming **10-fold cross-validation**.\n",
    "\n",
    "Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data that were not used during the training of the model.\n",
    "\n",
    "It is a popular method because it is simple to understand and because it generally results in a less biased or less optimistic estimate of the model skill than other methods, such as a simple train/test split.\n",
    "\n",
    "The general procedure is as follows:\n",
    "\n",
    ">- Shuffle the dataset randomly.\n",
    ">- Split the dataset into k groups\n",
    ">  - For each unique group:\n",
    ">        - Take the group as a hold out or test data set\n",
    ">        - Take the remaining groups as a training data set\n",
    ">        - Fit a model on the training set and evaluate it on the test set\n",
    ">        - Retain the evaluation score and discard the model\n",
    ">   - Summarize the skill of the model using the sample of model evaluation scores\n",
    "\n",
    "Importantly, each observation in the data sample is assigned to an individual group and stays in that group for the duration of the procedure. This means that each sample is given the opportunity to be used in the **hold out** set **1** time and used to train the model **k-1** times.\n",
    "\n",
    "Each of these hold outs is called a fold, hence the name of the method\n",
    "\n",
    "```{admonition} Quote\n",
    "\"This approach involves randomly dividing the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining k − 1 folds.\"\n",
    "\n",
    "pp. 181 [An Introduction to Statistical Learning: with Applications in R (2021)](http://54.243.252.9/ce-5319-webroot/3-Readings/IntoductiontoStatisticalLearning.pdf)\n",
    "\n",
    "```\n",
    "\n",
    "It is also important that any preparation of the data prior to fitting the model occur on the CV-assigned training dataset within the loop rather than on the broader data set. This requirement also applies to any tuning of hyperparameters. A failure to perform these operations within the loop may result in data leakage and an optimistic estimate of the model skill.\n",
    "\n",
    "```{admonition} Quote\n",
    "\"Despite the best efforts of statistical methodologists, users frequently invalidate their results by inadvertently peeking at the test data.\"\n",
    "\n",
    "[Artificial Intelligence a Modern Approach (2020)](http://54.243.252.9/ce-5319-webroot/3-Readings/aima2010.pdf)\n",
    "\n",
    "```\n",
    "\n",
    "The results of a **k-fold** cross-validation run are often summarized with the mean of the model skill scores. It is also good practice to include a measure of the variance of the skill scores, such as the standard deviation or standard error.\n",
    "\n",
    "### Configuration of k\n",
    "\n",
    "The **k** value must be chosen carefully for your data sample.\n",
    "\n",
    "A poorly chosen value for **k** may result in a misrepresentative idea of the skill of the model, such as a score with a high variance (that may change a lot based on the data used to fit the model), or a high bias, (such as an overestimate of the skill of the model).\n",
    "\n",
    "Three common tactics for choosing a value for k are as follows:\n",
    "\n",
    "- **Representative**: The value for k is chosen such that each train/test group of data samples is large enough to be statistically representative of the broader dataset.\n",
    "- **k=5; or 10**: The value for k is fixed to 5 or 10, a value that has been found through experimentation to generally result in a model skill estimate with low bias a modest variance.\n",
    "- **k=n**: The value for k is fixed to n, where n is the size of the dataset to give each test sample an opportunity to be used in the hold out dataset. This approach is called leave-one-out cross-validation.\n",
    "\n",
    "```{admonition} Quote\n",
    "\"The choice of k is usually 5 or 10, but there is no formal rule. As k gets larger, the difference in size between the training set and the resampling subsets gets smaller. As this difference decreases, the bias of the technique becomes smaller\"\n",
    "\n",
    "[Applied Predictive Modeling (2018)](http://54.243.252.9ce-5319-webroot/3-Readings/applied-predictive-modeling-max-kuhn-kjell-johnson_1518.pdf)\n",
    "\n",
    "```\n",
    "\n",
    "Values of k=5 or 10 are very common in the field of applied machine learning, and are recommended if you are struggling to choose a value for your dataset.\n",
    "\n",
    "```{admonition} Quote\n",
    "\"To summarize, there is a bias-variance trade-off associated with the choice of k in k-fold cross-validation. Typically, given these considerations, one performs k-fold cross-validation using k = 5 or k = 10, as these values have been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance.\"\n",
    "\n",
    "— Page 184, [An Introduction to Statistical Learning, 2013](http://54.243.252.9/ce-5319-webroot/3-Readings/IntoductiontoStatisticalLearning.pdf)\n",
    "```\n",
    "\n",
    "If a value for k is chosen that does not evenly split the data sample, then one group will contain a remainder of the examples. It is preferable to split the data sample into k groups with the same number of samples, such that the sample of model skill scores are all equivalent.\n",
    "\n",
    ":::{note}\n",
    "\n",
    "Here is an example to illustrate the \"folding\" steps\n",
    "\n",
    "![](k-fold-example.png)\n",
    "\n",
    ":::\n",
    "\n",
    "For more on how to configure **k-fold** cross-validation, see the tutorial:[how-to-configure-k-fold-cross-validation](https://machinelearningmastery.com/how-to-configure-k-fold-cross-validation/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
